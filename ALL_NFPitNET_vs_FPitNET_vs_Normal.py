# -*- coding: utf-8 -*-
"""all NFPA vs HSPA vs Normal Axials

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MwLfyhKRRKAeUEQ4vviLRvDSNqoHacaJ
"""

!pip install --quiet git+https://github.com/jacobgil/pytorch-grad-cam.git

import os, random, numpy as np, matplotlib.pyplot as plt, seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms, models
from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
from collections import Counter

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/MLClivusProject/CompleteDataset/All/Axial'
#may change to evaluate sagittal/coronal images

import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

full_dataset = datasets.ImageFolder(data_dir, transform=transform)
class_names = full_dataset.classes  # ['FPitNET', 'NFPitNET', 'Normal']
targets = [label for _, label in full_dataset]

kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
train_idx, val_idx = next(kf.split(np.zeros(len(targets)), targets))
train_dataset = Subset(full_dataset, train_idx)
val_dataset = Subset(full_dataset, val_idx)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

def train_model(model, optimizer, criterion, loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss, correct = 0, 0
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            correct += (outputs.argmax(1) == labels).sum().item()
        print(f"Epoch {epoch+1}: Loss={total_loss:.4f}, Acc={correct/len(loader.dataset):.4f}")

resnet = models.resnet18(weights="IMAGENET1K_V1")
resnet.fc = nn.Linear(resnet.fc.in_features, 3)
resnet = resnet.to(device)

counts = Counter(targets)
weights = [sum(counts.values()) / counts[i] for i in range(3)]
criterion_resnet = nn.CrossEntropyLoss(weight=torch.tensor(weights).to(device))
optimizer_resnet = torch.optim.Adam(resnet.parameters(), lr=1e-4)

print("\nðŸ”§ Training ResNet18:")
train_model(resnet, optimizer_resnet, criterion_resnet, train_loader)

def create_vit_model():
    weights = ViT_B_16_Weights.DEFAULT
    model = vit_b_16(weights=weights)
    model.heads.head = nn.Linear(model.heads.head.in_features, 3)
    return model.to(device)

vit = create_vit_model()
optimizer_vit = torch.optim.Adam(vit.parameters(), lr=1e-4)
criterion_vit = nn.CrossEntropyLoss()

print("\nðŸ”§ Training ViT:")
train_model(vit, optimizer_vit, criterion_vit, train_loader)

def evaluate_model(model, loader, name):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            preds = outputs.argmax(1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    print(f"\n Evaluation: {name}")
    print(classification_report(all_labels, all_preds, target_names=class_names))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

evaluate_model(resnet, val_loader, "ResNet18")
evaluate_model(vit, val_loader, "ViT")



from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize

def evaluate_model(model, loader, name):
    model.eval()
    all_preds, all_labels, all_probs = [], [], []

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)

            all_preds.extend(outputs.argmax(1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    y_true = np.array(all_labels)
    y_score = np.array(all_probs)
    present_labels = np.unique(y_true)
    n_present = len(present_labels)

    id_to_name = {i: class_names[i] for i in range(len(class_names))}
    present_names = [id_to_name[i] for i in present_labels]

    print(f"\n Evaluation: {name}")
    print(classification_report(y_true, np.array(all_preds), target_names=class_names))

    cm = confusion_matrix(y_true, np.array(all_preds), labels=present_labels)
    plt.figure(figsize=(5.5, 4.5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=present_names, yticklabels=present_names)
    plt.title(f"{name} â€¢ Confusion Matrix")
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout(); plt.show()

    if n_present < 2:
        print("ROC not available (validation set contains only one class).")
        return

    if n_present == 2:
        pos_label = present_labels[1]
        fpr, tpr, _ = roc_curve((y_true == pos_label).astype(int), y_score[:, pos_label],
                                drop_intermediate=False)
        auc_val = auc(fpr, tpr)
        print(f"AUC (pos={id_to_name[pos_label]}): {auc_val:.3f}")

        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, lw=2, label=f"ROC (AUC={auc_val:.3f})")
        plt.plot([0, 1], [0, 1], linestyle='--', lw=1, label='Chance')
        plt.xlim([0, 1]); plt.ylim([0, 1.05])
        plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
        plt.title(f"{name} â€¢ ROC Curve")
        plt.legend(loc="lower right")
        plt.tight_layout(); plt.show()

    else:
        y_bin = label_binarize(y_true, classes=present_labels)

        fpr, tpr, roc_auc = {}, {}, {}
        valid_idxs = []
        for j, lab in enumerate(present_labels):
            positives = y_bin[:, j].sum()
            if positives == 0 or positives == len(y_bin):
                continue
            fpr[lab], tpr[lab], _ = roc_curve(y_bin[:, j], y_score[:, lab],
                                              drop_intermediate=False)
            roc_auc[lab] = auc(fpr[lab], tpr[lab])
            valid_idxs.append(lab)

        if not valid_idxs:
            print("ROC not available (no usable classes in this split).")
            return

        Y_valid = np.column_stack([y_bin[:, np.where(present_labels == lab)[0][0]] for lab in valid_idxs])
        S_valid = np.column_stack([y_score[:, lab] for lab in valid_idxs])
        fpr["micro"], tpr["micro"], _ = roc_curve(Y_valid.ravel(), S_valid.ravel(),
                                                  drop_intermediate=False)
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        all_fpr = np.unique(np.concatenate([fpr[lab] for lab in valid_idxs]))
        mean_tpr = np.zeros_like(all_fpr)
        for lab in valid_idxs:
            mean_tpr += np.interp(all_fpr, fpr[lab], tpr[lab])
        mean_tpr /= len(valid_idxs)
        fpr["macro"], tpr["macro"] = all_fpr, mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        try:
            auc_macro = roc_auc_score(y_true, y_score, multi_class='ovr', average='macro',
                                      labels=present_labels)
            auc_weighted = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted',
                                         labels=present_labels)
            print(f"AUC (macro OvR): {auc_macro:.3f} | AUC (weighted OvR): {auc_weighted:.3f}")
        except ValueError:
            pass

        plt.figure(figsize=(7, 6))
        plt.plot(fpr["micro"], tpr["micro"], lw=2,
                 label=f"micro-average (AUC={roc_auc['micro']:.3f})")
        plt.plot(fpr["macro"], tpr["macro"], lw=2,
                 label=f"macro-average (AUC={roc_auc['macro']:.3f})")
        for lab in valid_idxs:
            plt.plot(fpr[lab], tpr[lab], lw=1.6,
                     label=f"{id_to_name[lab]} (AUC={roc_auc[lab]:.3f})")
        plt.plot([0, 1], [0, 1], linestyle='--', lw=1, label='Chance')
        plt.xlim([0, 1]); plt.ylim([0, 1.05])
        plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
        plt.title(f"{name} â€¢ ROC Curves (OvR)")
        plt.legend(loc="lower right"); plt.tight_layout(); plt.show()

evaluate_model(resnet, val_loader, "ResNet18")
evaluate_model(vit, val_loader, "ViT")

def reshape_transform(tensor, height=14, width=14):
    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, -1)
    return result.permute(0, 3, 1, 2)

def generate_gradcam(model, img_tensor, pred_label, is_vit=False):
    input_tensor = img_tensor.unsqueeze(0).to(device)
    if is_vit:
        cam = GradCAM(model=model, target_layers=[model.encoder.layers[-1].ln_1], reshape_transform=reshape_transform)
    else:
        cam = GradCAM(model=model, target_layers=[model.layer4[-1]])
    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(pred_label)])[0]
    input_image = img_tensor.permute(1, 2, 0).cpu().numpy()
    input_image = (input_image * 0.5) + 0.5
    return show_cam_on_image(input_image, grayscale_cam, use_rgb=True)

def plot_gradcam_grid(model, name, is_vit=False):
    model.eval()
    samples_per_class = 2
    images_by_class = [[s for s in val_dataset if s[1] == i][:samples_per_class] for i in range(3)]

    fig, axes = plt.subplots(3, samples_per_class, figsize=(samples_per_class * 5, 12))
    for row, class_samples in enumerate(images_by_class):
        for col, (img_tensor, true_label) in enumerate(class_samples):
            with torch.no_grad():
                pred = model(img_tensor.unsqueeze(0).to(device)).argmax(1).item()
            cam_img = generate_gradcam(model, img_tensor, pred, is_vit)
            axes[row][col].imshow(cam_img)
            axes[row][col].set_title(f"True: {class_names[true_label]} | Pred: {class_names[pred]}")
            axes[row][col].axis('off')
    plt.suptitle(f"{name} Grad-CAMs by Class", fontsize=16)
    plt.tight_layout()
    plt.show()

plot_gradcam_grid(resnet, "ResNet18", is_vit=False)
plot_gradcam_grid(vit, "ViT", is_vit=True)

from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
def evaluate_model(model, loader, name):
    model.eval()
    all_preds, all_labels, all_probs = [], [], []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            preds = outputs.argmax(1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    print(f"\n Evaluation: {name}")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.show()

    y_true = np.array(all_labels)
    y_score = np.array(all_probs)
    n_classes = len(class_names)

    if n_classes == 2:
        if len(np.unique(y_true)) < 2:
            print("ROC curve not available: validation split has only one class present.")
            return
        fpr, tpr, _ = roc_curve(y_true, y_score[:, 1])
        auc_val = auc(fpr, tpr)
        print(f"AUC: {auc_val:.3f}")

        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, lw=2, label=f'ROC (AUC={auc_val:.3f})')
        plt.plot([0, 1], [0, 1], lw=1, linestyle='--', label='Chance')
        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
        plt.title(f'{name} ROC Curve'); plt.legend(loc='lower right')
        plt.show()

    else:
        classes_idx = list(range(n_classes))
        y_bin = label_binarize(y_true, classes=classes_idx)

        fpr, tpr, roc_auc = {}, {}, {}
        valid_classes = []
        for i in classes_idx:
            if y_bin[:, i].sum() == 0 or y_bin[:, i].sum() == len(y_bin):
                continue
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            valid_classes.append(i)

        if not valid_classes:
            print("ROC curves not available: validation split lacks class variety.")
            return

        fpr["micro"], tpr["micro"], _ = roc_curve(y_bin[:, valid_classes].ravel(),
                                                  y_score[:, valid_classes].ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        all_fpr = np.unique(np.concatenate([fpr[i] for i in valid_classes]))
        mean_tpr = np.zeros_like(all_fpr)
        for i in valid_classes:
            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
        mean_tpr /= len(valid_classes)
        fpr["macro"], tpr["macro"] = all_fpr, mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        try:
            auc_macro = roc_auc_score(y_true, y_score, multi_class='ovr', average='macro')
            auc_weighted = roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')
            print(f"AUC (macro OvR): {auc_macro:.3f} | AUC (weighted OvR): {auc_weighted:.3f}")
            for i in valid_classes:
                print(f"  AUC[{class_names[i]}]: {roc_auc[i]:.3f}")
        except ValueError:
            pass

        plt.figure(figsize=(7, 6))
        plt.plot(fpr["micro"], tpr["micro"], lw=2, label=f'micro-average (AUC={roc_auc["micro"]:.3f})')
        plt.plot(fpr["macro"], tpr["macro"], lw=2, label=f'macro-average (AUC={roc_auc["macro"]:.3f})')

        for i in valid_classes:
            plt.plot(fpr[i], tpr[i], lw=1.5, label=f'{class_names[i]} (AUC={roc_auc[i]:.3f})')

        plt.plot([0, 1], [0, 1], linestyle='--', lw=1, label='Chance')
        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
        plt.title(f'{name} ROC Curves (OvR)')
        plt.legend(loc='lower right')
        plt.show()